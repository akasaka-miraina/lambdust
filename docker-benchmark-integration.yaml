# Docker Benchmark Integration Configuration
# This file defines how Lambdust native benchmark results integrate
# with Docker-based benchmark frameworks for cross-implementation comparison

version: "1.0"
integration_name: "lambdust-native-benchmarks"

# Metadata about the native benchmark system
native_system:
  name: "Lambdust Native Benchmark Runner"
  version: "1.0.0"
  language: "Rust"
  benchmark_format: "lambdust_native_v1"
  output_formats: ["json", "csv"]
  
# Benchmark categories and their Docker equivalents
category_mapping:
  # Native category -> Docker benchmark equivalent
  "Arithmetic Operations": "numeric-operations"
  "List Operations": "list-processing"
  "Recursion Performance": "recursive-algorithms"
  "Memory Allocation": "memory-management"
  "Function Calls": "function-overhead"
  "Realistic Programs": "scheme-programs"

# Metric mappings between native and Docker formats
metric_mapping:
  # Native metric -> Docker metric
  "ops_per_second": "operations_per_second"
  "total_time_ms": "total_execution_time_ms"
  "memory_usage_mb": "memory_consumption_mb"
  "iterations": "iteration_count"
  "overall_performance_score": "composite_performance_score"
  
# Result transformation configuration
result_transformation:
  # How to convert native results to Docker-compatible format
  scaling_factors:
    time_units: "milliseconds"  # Native uses ms, Docker might use different units
    memory_units: "megabytes"   # Native uses MB
    throughput_units: "ops_per_second"
  
  # Statistical metrics transformation
  statistical_mapping:
    "mean_time_ms": "average_execution_time"
    "median_time_ms": "median_execution_time"
    "std_deviation_ms": "execution_time_stddev"
    "p95_time_ms": "p95_execution_time"
    "p99_time_ms": "p99_execution_time"

# Docker-compatible output structure
docker_output_schema:
  version: "docker_benchmark_v2"
  fields:
    - name: "timestamp"
      type: "iso8601_datetime"
      source: "native.timestamp"
    
    - name: "system_info"
      type: "object"
      fields:
        - name: "implementation"
          value: "lambdust"
        - name: "version"
          source: "native.system_info.lambdust_version"
        - name: "platform"
          source: "native.system_info.platform"
        - name: "cpu_cores"
          source: "native.system_info.cpu_count"
    
    - name: "benchmark_results"
      type: "array"
      element_schema:
        - name: "category"
          source: "native.category.name"
          transform: "category_mapping"
        - name: "test_name"
          source: "native.result.test_name"
        - name: "performance_metrics"
          type: "object"
          fields:
            - name: "operations_per_second"
              source: "native.result.ops_per_second"
            - name: "execution_time_ms"
              source: "native.result.total_time_ms"
            - name: "memory_usage_mb"
              source: "native.result.memory_usage_mb"
            - name: "statistical_summary"
              source: "native.result.statistical_metrics"
              transform: "statistical_mapping"

# Benchmark comparison configuration
comparison_framework:
  # How native results compare to other Scheme implementations
  baseline_implementations:
    - name: "guile"
      docker_image: "scheme-benchmarks/guile"
      result_format: "docker_benchmark_v2"
    - name: "racket"
      docker_image: "scheme-benchmarks/racket"
      result_format: "docker_benchmark_v2"
    - name: "chicken"
      docker_image: "scheme-benchmarks/chicken"
      result_format: "docker_benchmark_v2"
  
  # Comparison metrics
  comparison_metrics:
    - name: "relative_performance"
      calculation: "lambdust_ops_per_sec / baseline_ops_per_sec"
    - name: "memory_efficiency"
      calculation: "baseline_memory_mb / lambdust_memory_mb"
    - name: "startup_overhead"
      calculation: "lambdust_startup_time - baseline_startup_time"

# Integration scripts and commands
integration_commands:
  # Convert native results to Docker format
  convert_to_docker_format: |
    python3 scripts/convert_native_to_docker.py \
      --input native_results.json \
      --output docker_compatible_results.json \
      --config docker-benchmark-integration.yaml
  
  # Run comparison with Docker benchmarks
  run_docker_comparison: |
    docker-compose -f docker-compose.benchmarks.yml run comparison \
      --lambdust-results docker_compatible_results.json \
      --baseline-implementations guile,racket,chicken \
      --output comparison_report.html
  
  # Generate comprehensive cross-implementation report
  generate_cross_impl_report: |
    python3 scripts/generate_cross_implementation_report.py \
      --native-results native_results.json \
      --docker-results docker_results/ \
      --output cross_implementation_analysis.html

# File path conventions for Docker integration
file_conventions:
  # Where to place files for Docker pickup
  docker_input_directory: "./benchmark-results/docker-compatible/input/"
  docker_output_directory: "./benchmark-results/docker-compatible/output/"
  
  # Expected file naming patterns
  naming_patterns:
    native_results: "lambdust_benchmark_results_{timestamp}.json"
    docker_converted: "lambdust_docker_format_{timestamp}.json"
    comparison_report: "cross_implementation_comparison_{timestamp}.html"
    
  # File retention policy
  retention:
    native_results_days: 30
    docker_results_days: 7
    comparison_reports_days: 14

# Performance thresholds for automated alerts
performance_thresholds:
  # Regression detection thresholds
  regression_alerts:
    overall_performance_drop: 10  # Percent drop to trigger alert
    category_performance_drop: 15
    memory_usage_increase: 20
  
  # Competitive performance targets
  competitive_targets:
    arithmetic_ops_vs_racket: 0.8  # Target 80% of Racket performance
    list_ops_vs_guile: 0.9        # Target 90% of Guile performance
    memory_usage_vs_chicken: 1.2   # Target max 120% of Chicken memory usage

# CI/CD Integration
ci_cd_integration:
  # GitHub Actions integration
  github_actions:
    benchmark_on_pr: true
    regression_check_threshold: 5  # Percent
    comparison_on_release: true
  
  # Jenkins integration
  jenkins:
    pipeline_trigger: "performance-regression-check"
    baseline_update_frequency: "weekly"
  
  # Docker Registry settings
  docker_registry:
    push_results: true
    registry_url: "benchmark-results.company.com"
    namespace: "lambdust-benchmarks"

# Visualization and Reporting
visualization:
  # Chart types for different metrics
  chart_types:
    performance_trends: "line_chart"
    category_comparison: "radar_chart"
    implementation_comparison: "bar_chart"
    memory_usage: "area_chart"
  
  # Dashboard configuration
  dashboard:
    auto_refresh_seconds: 300
    default_time_range: "7_days"
    comparison_implementations: ["guile", "racket", "chicken"]
    
  # Report formats
  report_formats:
    - format: "html"
      template: "comprehensive_report.html.j2"
    - format: "pdf"
      template: "executive_summary.pdf.j2"
    - format: "json"
      schema: "docker_benchmark_v2"

# Quality gates for release process
quality_gates:
  # Minimum performance requirements for release
  release_criteria:
    overall_score_minimum: 70
    no_major_regressions: true
    memory_leak_detection: "passed"
    cross_implementation_competitive: true
  
  # Warning thresholds
  warning_thresholds:
    overall_score_warning: 60
    category_regression_warning: 10
    memory_usage_warning: 150  # MB

# Documentation and Help
documentation:
  integration_guide: "docs/docker-benchmark-integration.md"
  troubleshooting: "docs/benchmark-troubleshooting.md"
  api_reference: "docs/benchmark-api-reference.md"
  
help_commands:
  check_integration: |
    python3 scripts/check_docker_integration.py --config docker-benchmark-integration.yaml
  
  validate_results: |
    python3 scripts/validate_benchmark_results.py --file results.json --schema docker_benchmark_v2
  
  debug_conversion: |
    python3 scripts/debug_format_conversion.py --input native.json --output docker.json --verbose